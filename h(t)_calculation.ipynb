{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify t4 and category by percentage change and day window values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alaska'\n",
      " 'Alaska' 'Alaska' 'Alaska' 'Alaska' 'Alaska' 'Arizona' 'Arizona'\n",
      " 'Arizona' 'Arizona' 'Arizona' 'Arizona' 'Arkansas' 'Arkansas' 'Arkansas'\n",
      " 'Arkansas' 'Arkansas' 'Arkansas' 'California' 'California' 'California'\n",
      " 'California' 'California' 'California' 'Colorado' 'Colorado' 'Colorado'\n",
      " 'Colorado' 'Colorado' 'Colorado' 'Connecticut' 'Connecticut'\n",
      " 'Connecticut' 'Connecticut' 'Connecticut' 'Connecticut' 'Delaware'\n",
      " 'Delaware' 'Delaware' 'Delaware' 'Delaware' 'Delaware'\n",
      " 'District of Columbia' 'District of Columbia' 'District of Columbia'\n",
      " 'District of Columbia' 'District of Columbia' 'District of Columbia'\n",
      " 'Florida' 'Florida' 'Florida' 'Florida' 'Florida' 'Florida' 'Georgia'\n",
      " 'Georgia' 'Georgia' 'Georgia' 'Georgia' 'Georgia' 'Hawaii' 'Hawaii'\n",
      " 'Hawaii' 'Hawaii' 'Hawaii' 'Hawaii' 'Idaho' 'Idaho' 'Idaho' 'Idaho'\n",
      " 'Idaho' 'Idaho' 'Illinois' 'Illinois' 'Illinois' 'Illinois' 'Illinois'\n",
      " 'Illinois' 'Indiana' 'Indiana' 'Indiana' 'Indiana' 'Indiana' 'Indiana'\n",
      " 'Iowa' 'Iowa' 'Iowa' 'Iowa' 'Iowa' 'Iowa' 'Kansas' 'Kansas' 'Kansas'\n",
      " 'Kansas' 'Kansas' 'Kansas' 'Kentucky' 'Kentucky' 'Kentucky' 'Kentucky'\n",
      " 'Kentucky' 'Kentucky' 'Louisiana' 'Louisiana' 'Louisiana' 'Louisiana'\n",
      " 'Louisiana' 'Louisiana' 'Maine' 'Maine' 'Maine' 'Maine' 'Maine' 'Maine'\n",
      " 'Maryland' 'Maryland' 'Maryland' 'Maryland' 'Maryland' 'Maryland'\n",
      " 'Massachusetts' 'Massachusetts' 'Massachusetts' 'Massachusetts'\n",
      " 'Massachusetts' 'Massachusetts' 'Michigan' 'Michigan' 'Michigan'\n",
      " 'Michigan' 'Michigan' 'Michigan' 'Minnesota' 'Minnesota' 'Minnesota'\n",
      " 'Minnesota' 'Minnesota' 'Minnesota' 'Mississippi' 'Mississippi'\n",
      " 'Mississippi' 'Mississippi' 'Mississippi' 'Mississippi' 'Missouri'\n",
      " 'Missouri' 'Missouri' 'Missouri' 'Missouri' 'Missouri' 'Montana'\n",
      " 'Montana' 'Montana' 'Montana' 'Montana' 'Montana' 'Nebraska' 'Nebraska'\n",
      " 'Nebraska' 'Nebraska' 'Nebraska' 'Nebraska' 'Nevada' 'Nevada' 'Nevada'\n",
      " 'Nevada' 'Nevada' 'Nevada' 'New Hampshire' 'New Hampshire'\n",
      " 'New Hampshire' 'New Hampshire' 'New Hampshire' 'New Hampshire'\n",
      " 'New Jersey' 'New Jersey' 'New Jersey' 'New Jersey' 'New Jersey'\n",
      " 'New Jersey' 'New Mexico' 'New Mexico' 'New Mexico' 'New Mexico'\n",
      " 'New Mexico' 'New Mexico' 'New York' 'New York' 'New York' 'New York'\n",
      " 'New York' 'New York' 'North Carolina' 'North Carolina' 'North Carolina'\n",
      " 'North Carolina' 'North Carolina' 'North Carolina' 'North Dakota'\n",
      " 'North Dakota' 'North Dakota' 'North Dakota' 'North Dakota'\n",
      " 'North Dakota' 'Ohio' 'Ohio' 'Ohio' 'Ohio' 'Ohio' 'Ohio' 'Oklahoma'\n",
      " 'Oklahoma' 'Oklahoma' 'Oklahoma' 'Oklahoma' 'Oklahoma' 'Oregon' 'Oregon'\n",
      " 'Oregon' 'Oregon' 'Oregon' 'Oregon' 'Pennsylvania' 'Pennsylvania'\n",
      " 'Pennsylvania' 'Pennsylvania' 'Pennsylvania' 'Pennsylvania'\n",
      " 'Rhode Island' 'Rhode Island' 'Rhode Island' 'Rhode Island'\n",
      " 'Rhode Island' 'Rhode Island' 'South Carolina' 'South Carolina'\n",
      " 'South Carolina' 'South Carolina' 'South Carolina' 'South Carolina'\n",
      " 'South Dakota' 'South Dakota' 'South Dakota' 'South Dakota'\n",
      " 'South Dakota' 'South Dakota' 'Tennessee' 'Tennessee' 'Tennessee'\n",
      " 'Tennessee' 'Tennessee' 'Tennessee' 'Texas' 'Texas' 'Texas' 'Texas'\n",
      " 'Texas' 'Texas' 'US' 'US' 'US' 'US' 'US' 'US' 'Utah' 'Utah' 'Utah' 'Utah'\n",
      " 'Utah' 'Utah' 'Vermont' 'Vermont' 'Vermont' 'Vermont' 'Vermont' 'Vermont'\n",
      " 'Virginia' 'Virginia' 'Virginia' 'Virginia' 'Virginia' 'Virginia'\n",
      " 'Washington' 'Washington' 'Washington' 'Washington' 'Washington'\n",
      " 'Washington' 'West Virginia' 'West Virginia' 'West Virginia'\n",
      " 'West Virginia' 'West Virginia' 'West Virginia' 'Wisconsin' 'Wisconsin'\n",
      " 'Wisconsin' 'Wisconsin' 'Wisconsin' 'Wisconsin' 'Wyoming' 'Wyoming'\n",
      " 'Wyoming' 'Wyoming' 'Wyoming' 'Wyoming']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eddie\\AppData\\Local\\Temp\\ipykernel_28124\\3171826867.py:83: FutureWarning: Passing 'suffixes' which cause duplicate columns {'PG_x', 'PN_x', 'PL_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  result_df = pd.merge(result_df, category_relative_percentages.reset_index(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>mobility_type</th>\n",
       "      <th>x_time</th>\n",
       "      <th>x_date</th>\n",
       "      <th>y_ridership_change</th>\n",
       "      <th>phase</th>\n",
       "      <th>t4_1pct_5days</th>\n",
       "      <th>t4_1pct_7days</th>\n",
       "      <th>t4_1pct_14days</th>\n",
       "      <th>t4_5pct_5days</th>\n",
       "      <th>t4_5pct_7days</th>\n",
       "      <th>t4_5pct_14days</th>\n",
       "      <th>t4_1pct_5days_category</th>\n",
       "      <th>t4_1pct_7days_category</th>\n",
       "      <th>t4_1pct_14days_category</th>\n",
       "      <th>t4_5pct_5days_category</th>\n",
       "      <th>t4_5pct_7days_category</th>\n",
       "      <th>t4_5pct_14days_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>grocery_and_pharmacy_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.454906205, -...</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>parks_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.227272727, 5....</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>PG</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>residential_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.187229437, 1....</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PN</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>retail_and_recreation_percent_change_from_base...</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.392135642, 1....</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>transit_stations_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.971500722, 2....</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>parks_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.761904762, 6....</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>residential_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.80952381, 0.7...</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>retail_and_recreation_percent_change_from_base...</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.619047619, 3....</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>transit_stations_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.142857143, 5....</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>workplaces_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.666666667, -...</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       state                                      mobility_type  \\\n",
       "0    Alabama  grocery_and_pharmacy_percent_change_from_baseline   \n",
       "1    Alabama                 parks_percent_change_from_baseline   \n",
       "2    Alabama           residential_percent_change_from_baseline   \n",
       "3    Alabama  retail_and_recreation_percent_change_from_base...   \n",
       "4    Alabama      transit_stations_percent_change_from_baseline   \n",
       "..       ...                                                ...   \n",
       "307  Wyoming                 parks_percent_change_from_baseline   \n",
       "308  Wyoming           residential_percent_change_from_baseline   \n",
       "309  Wyoming  retail_and_recreation_percent_change_from_base...   \n",
       "310  Wyoming      transit_stations_percent_change_from_baseline   \n",
       "311  Wyoming            workplaces_percent_change_from_baseline   \n",
       "\n",
       "                                                x_time  \\\n",
       "0    [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "1    [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "2    [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "3    [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "4    [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "..                                                 ...   \n",
       "307  [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "308  [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "309  [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "310  [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "311  [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "\n",
       "                                                x_date  \\\n",
       "0    [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "1    [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "2    [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "3    [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "4    [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "..                                                 ...   \n",
       "307  [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "308  [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "309  [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "310  [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "311  [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "\n",
       "                                    y_ridership_change  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.454906205, -...   \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.227272727, 5....   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.187229437, 1....   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.392135642, 1....   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.971500722, 2....   \n",
       "..                                                 ...   \n",
       "307  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.761904762, 6....   \n",
       "308  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.80952381, 0.7...   \n",
       "309  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.619047619, 3....   \n",
       "310  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.142857143, 5....   \n",
       "311  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.666666667, -...   \n",
       "\n",
       "                                                 phase  t4_1pct_5days  \\\n",
       "0    [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...            NaN   \n",
       "1    [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...           96.0   \n",
       "2    [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...           93.0   \n",
       "3    [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...            NaN   \n",
       "4    [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...            NaN   \n",
       "..                                                 ...            ...   \n",
       "307  [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...          159.0   \n",
       "308  [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...            NaN   \n",
       "309  [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...            NaN   \n",
       "310  [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...            NaN   \n",
       "311  [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...           98.0   \n",
       "\n",
       "     t4_1pct_7days  t4_1pct_14days  t4_5pct_5days  t4_5pct_7days  \\\n",
       "0              NaN             NaN            NaN            NaN   \n",
       "1              NaN             NaN           87.0           87.0   \n",
       "2             93.0             NaN           75.0           75.0   \n",
       "3              NaN             NaN           81.0           81.0   \n",
       "4              NaN             NaN           86.0           86.0   \n",
       "..             ...             ...            ...            ...   \n",
       "307          159.0           159.0           75.0           75.0   \n",
       "308            NaN             NaN           75.0           75.0   \n",
       "309            NaN             NaN           75.0           75.0   \n",
       "310            NaN             NaN           75.0           75.0   \n",
       "311          120.0           120.0           75.0           75.0   \n",
       "\n",
       "     t4_5pct_14days t4_1pct_5days_category t4_1pct_7days_category  \\\n",
       "0               NaN                     PN                     PN   \n",
       "1              87.0                     PG                     PN   \n",
       "2              75.0                     PG                     PG   \n",
       "3              81.0                     PN                     PN   \n",
       "4              86.0                     PN                     PN   \n",
       "..              ...                    ...                    ...   \n",
       "307            75.0                     PG                     PG   \n",
       "308            75.0                     PN                     PN   \n",
       "309             NaN                     PN                     PN   \n",
       "310           120.0                     PN                     PN   \n",
       "311            75.0                     PL                     PL   \n",
       "\n",
       "    t4_1pct_14days_category t4_5pct_5days_category t4_5pct_7days_category  \\\n",
       "0                        PN                     PN                     PN   \n",
       "1                        PN                     PG                     PG   \n",
       "2                        PN                     PG                     PG   \n",
       "3                        PN                     PL                     PL   \n",
       "4                        PN                     PL                     PL   \n",
       "..                      ...                    ...                    ...   \n",
       "307                      PG                     PG                     PG   \n",
       "308                      PN                     PG                     PG   \n",
       "309                      PN                     PL                     PL   \n",
       "310                      PN                     PL                     PL   \n",
       "311                      PL                     PL                     PL   \n",
       "\n",
       "    t4_5pct_14days_category  \n",
       "0                        PN  \n",
       "1                        PG  \n",
       "2                        PG  \n",
       "3                        PL  \n",
       "4                        PL  \n",
       "..                      ...  \n",
       "307                      PG  \n",
       "308                      PG  \n",
       "309                      PN  \n",
       "310                      PG  \n",
       "311                      PL  \n",
       "\n",
       "[306 rows x 18 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classify t4 and category\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv('Datasets/dataset_mobility.csv')\n",
    "\n",
    "grouped_df = dataset.groupby(['state', 'mobility_type']).agg({\n",
    "    'x_time': list,\n",
    "    'x_date': list,\n",
    "    'y_ridership_change': list,\n",
    "    'phase': list,\n",
    "}).reset_index()\n",
    "\n",
    "dataset = grouped_df\n",
    "\n",
    "\n",
    "def find_steady_state(y_ridership_change, threshold, window):\n",
    "    \"\"\"Find steady state based on raw data without smoothing.\"\"\"\n",
    "    consecutive_count = 0\n",
    "    for i in range(75, len(y_ridership_change) - 1):\n",
    "        # Small term added to avoid zero division\n",
    "        change = (y_ridership_change[i] - y_ridership_change[i-1]\n",
    "                  ) / (y_ridership_change[i-1] + 1e-10)\n",
    "        if abs(change) <= threshold:\n",
    "            consecutive_count += 1\n",
    "            if consecutive_count >= window:\n",
    "                return i - window + 1  # Return the starting point of the steady state\n",
    "        else:\n",
    "            consecutive_count = 0\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_trajectory(row, thresholds=[0.01, 0.05], windows=[5, 7, 14]):\n",
    "    y_changes = list(row['y_ridership_change'])\n",
    "    smoothed_y_changes = savgol_filter(\n",
    "        y_changes, window_length=25, polyorder=3)\n",
    "\n",
    "    t4_values = {}\n",
    "    for threshold in thresholds:\n",
    "        for window in windows:\n",
    "            t4_key = f't4_{int(threshold*100)}pct_{window}days'\n",
    "            t4_values[t4_key] = find_steady_state(\n",
    "                smoothed_y_changes, threshold, window)\n",
    "\n",
    "    return pd.Series(list(t4_values.values()))\n",
    "\n",
    "\n",
    "new_columns = []\n",
    "thresholds = [0.01, 0.05]\n",
    "windows = [5, 7, 14]\n",
    "for threshold in thresholds:\n",
    "    for window in windows:\n",
    "        new_columns.append(f't4_{int(threshold*100)}pct_{window}days')\n",
    "\n",
    "dataset[new_columns] = dataset.apply(process_trajectory, axis=1)\n",
    "\n",
    "\n",
    "def determine_category(value):\n",
    "    if value > 0.05:\n",
    "        return \"PG\"\n",
    "    elif value < -0.05:\n",
    "        return \"PL\"\n",
    "    else:\n",
    "        return \"PN\"\n",
    "\n",
    "\n",
    "for col in new_columns:\n",
    "    dataset[f\"{col}_category\"] = dataset.apply(lambda row: determine_category(\n",
    "        row['y_ridership_change'][int(row[col])] if not pd.isna(row[col]) else 0), axis=1)\n",
    "result_df = dataset.groupby('mobility_type').size(\n",
    ").reset_index(name='count').drop('count', axis=1)\n",
    "\n",
    "\n",
    "for col in new_columns:\n",
    "    category_counts = dataset.groupby(\n",
    "        ['mobility_type', f\"{col}_category\"]).size().unstack().fillna(0)\n",
    "    category_relative_percentages = (\n",
    "        category_counts.T / category_counts.sum(axis=1)).T * 100\n",
    "\n",
    "    result_df = pd.merge(result_df, category_relative_percentages.reset_index(\n",
    "    ), on='mobility_type', how='left')\n",
    "\n",
    "    for category in [\"PG\", \"PL\", \"PN\"]:\n",
    "        # Calculate the average magnitude for this category\n",
    "        avg_mag = dataset[dataset[f\"{col}_category\"] == category].apply(lambda row: abs(\n",
    "            row['y_ridership_change'][int(row[col])] if not pd.isna(row[col]) else 0), axis=1).mean()\n",
    "        result_df[f\"{col}_avg_magnitude_{category}\"] = avg_mag\n",
    "\n",
    "result_df.fillna(0, inplace=True)\n",
    "\n",
    "#result_df.to_csv('result.csv', index=False)\n",
    "#dataset = dataset[dataset['t4_5pct_7days_category'] == 'PL']\n",
    "\n",
    "dataset1 = dataset\n",
    "print(np.array(dataset1['state']))\n",
    "dataset1 = dataset1[dataset1['state'] != 'US']\n",
    "dataset1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIN MAX and cap data so there is no inf value. Change Cap Value if needed. This method is work in progress as it is assuming quite a few factors. Feel free to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>mobility_type</th>\n",
       "      <th>x_time</th>\n",
       "      <th>x_date</th>\n",
       "      <th>y_ridership_change</th>\n",
       "      <th>phase</th>\n",
       "      <th>t4_1pct_5days</th>\n",
       "      <th>t4_1pct_7days</th>\n",
       "      <th>t4_1pct_14days</th>\n",
       "      <th>t4_5pct_5days</th>\n",
       "      <th>t4_5pct_7days</th>\n",
       "      <th>t4_5pct_14days</th>\n",
       "      <th>t4_1pct_5days_category</th>\n",
       "      <th>t4_1pct_7days_category</th>\n",
       "      <th>t4_1pct_14days_category</th>\n",
       "      <th>t4_5pct_5days_category</th>\n",
       "      <th>t4_5pct_7days_category</th>\n",
       "      <th>t4_5pct_14days_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>grocery_and_pharmacy_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.987115773973152, 0.987115773973152, 0.98711...</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>parks_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.987115773973152, 0.987115773973152, 0.98711...</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>PG</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>residential_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.987115773973152, 0.987115773973152, 0.98711...</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PN</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>retail_and_recreation_percent_change_from_base...</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.987115773973152, 0.987115773973152, 0.98711...</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>transit_stations_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.987115773973152, 0.987115773973152, 0.98711...</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>parks_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.987115773973152, 0.987115773973152, 0.98711...</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>residential_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.987115773973152, 0.987115773973152, 0.98711...</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "      <td>PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>retail_and_recreation_percent_change_from_base...</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.987115773973152, 0.987115773973152, 0.98711...</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>transit_stations_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.987115773973152, 0.987115773973152, 0.98711...</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PN</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>workplaces_percent_change_from_baseline</td>\n",
       "      <td>[44972, 44973, 44974, 44975, 44976, 44977, 449...</td>\n",
       "      <td>[15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...</td>\n",
       "      <td>[0.987115773973152, 0.987115773973152, 0.98711...</td>\n",
       "      <td>[D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       state                                      mobility_type  \\\n",
       "0    Alabama  grocery_and_pharmacy_percent_change_from_baseline   \n",
       "1    Alabama                 parks_percent_change_from_baseline   \n",
       "2    Alabama           residential_percent_change_from_baseline   \n",
       "3    Alabama  retail_and_recreation_percent_change_from_base...   \n",
       "4    Alabama      transit_stations_percent_change_from_baseline   \n",
       "..       ...                                                ...   \n",
       "307  Wyoming                 parks_percent_change_from_baseline   \n",
       "308  Wyoming           residential_percent_change_from_baseline   \n",
       "309  Wyoming  retail_and_recreation_percent_change_from_base...   \n",
       "310  Wyoming      transit_stations_percent_change_from_baseline   \n",
       "311  Wyoming            workplaces_percent_change_from_baseline   \n",
       "\n",
       "                                                x_time  \\\n",
       "0    [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "1    [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "2    [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "3    [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "4    [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "..                                                 ...   \n",
       "307  [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "308  [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "309  [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "310  [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "311  [44972, 44973, 44974, 44975, 44976, 44977, 449...   \n",
       "\n",
       "                                                x_date  \\\n",
       "0    [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "1    [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "2    [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "3    [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "4    [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "..                                                 ...   \n",
       "307  [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "308  [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "309  [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "310  [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "311  [15-Feb, 16-Feb, 17-Feb, 18-Feb, 19-Feb, 20-Fe...   \n",
       "\n",
       "                                    y_ridership_change  \\\n",
       "0    [0.987115773973152, 0.987115773973152, 0.98711...   \n",
       "1    [0.987115773973152, 0.987115773973152, 0.98711...   \n",
       "2    [0.987115773973152, 0.987115773973152, 0.98711...   \n",
       "3    [0.987115773973152, 0.987115773973152, 0.98711...   \n",
       "4    [0.987115773973152, 0.987115773973152, 0.98711...   \n",
       "..                                                 ...   \n",
       "307  [0.987115773973152, 0.987115773973152, 0.98711...   \n",
       "308  [0.987115773973152, 0.987115773973152, 0.98711...   \n",
       "309  [0.987115773973152, 0.987115773973152, 0.98711...   \n",
       "310  [0.987115773973152, 0.987115773973152, 0.98711...   \n",
       "311  [0.987115773973152, 0.987115773973152, 0.98711...   \n",
       "\n",
       "                                                 phase  t4_1pct_5days  \\\n",
       "0    [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...            NaN   \n",
       "1    [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...           96.0   \n",
       "2    [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...           93.0   \n",
       "3    [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...            NaN   \n",
       "4    [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...            NaN   \n",
       "..                                                 ...            ...   \n",
       "307  [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...          159.0   \n",
       "308  [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...            NaN   \n",
       "309  [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...            NaN   \n",
       "310  [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...            NaN   \n",
       "311  [D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, ...           98.0   \n",
       "\n",
       "     t4_1pct_7days  t4_1pct_14days  t4_5pct_5days  t4_5pct_7days  \\\n",
       "0              NaN             NaN            NaN            NaN   \n",
       "1              NaN             NaN           87.0           87.0   \n",
       "2             93.0             NaN           75.0           75.0   \n",
       "3              NaN             NaN           81.0           81.0   \n",
       "4              NaN             NaN           86.0           86.0   \n",
       "..             ...             ...            ...            ...   \n",
       "307          159.0           159.0           75.0           75.0   \n",
       "308            NaN             NaN           75.0           75.0   \n",
       "309            NaN             NaN           75.0           75.0   \n",
       "310            NaN             NaN           75.0           75.0   \n",
       "311          120.0           120.0           75.0           75.0   \n",
       "\n",
       "     t4_5pct_14days t4_1pct_5days_category t4_1pct_7days_category  \\\n",
       "0               NaN                     PN                     PN   \n",
       "1              87.0                     PG                     PN   \n",
       "2              75.0                     PG                     PG   \n",
       "3              81.0                     PN                     PN   \n",
       "4              86.0                     PN                     PN   \n",
       "..              ...                    ...                    ...   \n",
       "307            75.0                     PG                     PG   \n",
       "308            75.0                     PN                     PN   \n",
       "309             NaN                     PN                     PN   \n",
       "310           120.0                     PN                     PN   \n",
       "311            75.0                     PL                     PL   \n",
       "\n",
       "    t4_1pct_14days_category t4_5pct_5days_category t4_5pct_7days_category  \\\n",
       "0                        PN                     PN                     PN   \n",
       "1                        PN                     PG                     PG   \n",
       "2                        PN                     PG                     PG   \n",
       "3                        PN                     PL                     PL   \n",
       "4                        PN                     PL                     PL   \n",
       "..                      ...                    ...                    ...   \n",
       "307                      PG                     PG                     PG   \n",
       "308                      PN                     PG                     PG   \n",
       "309                      PN                     PL                     PL   \n",
       "310                      PN                     PL                     PL   \n",
       "311                      PL                     PL                     PL   \n",
       "\n",
       "    t4_5pct_14days_category  \n",
       "0                        PN  \n",
       "1                        PG  \n",
       "2                        PG  \n",
       "3                        PL  \n",
       "4                        PL  \n",
       "..                      ...  \n",
       "307                      PG  \n",
       "308                      PG  \n",
       "309                      PN  \n",
       "310                      PG  \n",
       "311                      PL  \n",
       "\n",
       "[312 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute min max and caps\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd  # Assuming you're using pandas for your dataset\n",
    "\n",
    "\n",
    "dataset1 = dataset\n",
    "\n",
    "\n",
    "def get_global_min_max(dataset, column_name):\n",
    "    all_values = [value for sublist in dataset[column_name].tolist()\n",
    "                  for value in sublist]\n",
    "    return min(all_values), max(all_values)\n",
    "\n",
    "\n",
    "def cap_values(lst, cap_value=-1):\n",
    "    # Convert numpy array to list if necessary\n",
    "    lst_list = lst.tolist() if isinstance(lst, np.ndarray) else lst\n",
    "\n",
    "    # Start from the minimum of the list\n",
    "    min_index = 30\n",
    "\n",
    "    # Check the number of values to the right of the minimum value\n",
    "    num_values_to_right = len(lst_list) - min_index - 1\n",
    "\n",
    "    if num_values_to_right >= 10:\n",
    "        # Cap values above the cap_value to be -cap_value\n",
    "        return [value if value < cap_value else cap_value for value in lst_list]\n",
    "    else:\n",
    "        # Return the original list\n",
    "        return [value if value > cap_value else cap_value for value in lst_list]\n",
    "\n",
    "\n",
    "def compute_derivative(lst):\n",
    "    return np.append([0], np.gradient(lst))\n",
    "\n",
    "\n",
    "def minmax_normalize(lst, min_val, max_val):\n",
    "    return [(x - min_val) / (0 - min_val) for x in lst]\n",
    "\n",
    "\n",
    "# Cap values in 'smoothed' to a minimum absolute value of 0.05\n",
    "#dataset['smoothed'] = dataset['smoothed'].apply(cap_values)\n",
    "dataset['y_ridership_change'] = dataset['y_ridership_change'].apply(cap_values)\n",
    "\n",
    "# Compute the derivative of the capped values\n",
    "'''dataset['derivative_smoothed'] = dataset['smoothed'].apply(\n",
    "    compute_derivative)\n",
    "dataset['derivative_ridership_change'] = dataset['y_ridership_change'].apply(\n",
    "    compute_derivative)'''\n",
    "\n",
    "# If you want to drop rows with NaN values in the selected column\n",
    "\n",
    "# Min-max normalization (commented out)\n",
    "\n",
    "'''global_min_smoothed, global_max_smoothed = get_global_min_max(\n",
    "    dataset, 'smoothed')'''\n",
    "global_min_change, global_max_change = get_global_min_max(\n",
    "    dataset, 'y_ridership_change')\n",
    "\n",
    "'''dataset['smoothed'] = dataset['smoothed'].apply(\n",
    "    minmax_normalize, args=(global_min_smoothed, global_max_smoothed))\n",
    "dataset['derivative_smoothed'] = dataset['smoothed'].apply(\n",
    "    compute_derivative)'''\n",
    "dataset['y_ridership_change'] = dataset['y_ridership_change'].apply(\n",
    "    minmax_normalize, args=(global_min_change, global_max_change))\n",
    "\n",
    "\n",
    "print(len(dataset1))\n",
    "\n",
    "\n",
    "'''plt.plot(dataset['derivative_smoothed'][2])\n",
    "plt.plot(dataset['smoothed'][2])'''\n",
    "'''dataset1 = dataset1[dataset1['mobility_type'] ==\n",
    "                    'workplaces_percent_change_from_baseline']'''\n",
    "dataset1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h(t), f(t), F(t), S(t) calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score  # Import r2_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "Qmax_values = {}\n",
    "alpha_values = {}\n",
    "beta_values = {}\n",
    "delta_values = {}\n",
    "initial_values = {}\n",
    "df = pd.DataFrame()\n",
    "res = []\n",
    "\n",
    "df_long_format = pd.DataFrame(columns=['state', 'mobility_type'])\n",
    "\n",
    "t4_cutoff = 't4_5pct_7days'\n",
    "dataset1 = dataset[dataset[f'{t4_cutoff}_category'] == 'PL']\n",
    "\n",
    "mobility_types = dataset1['mobility_type'].unique()\n",
    "\n",
    "for mobility_type in mobility_types:\n",
    "    dataset_filtered = dataset1[dataset1['mobility_type']\n",
    "                                == mobility_type].reset_index(drop=True)\n",
    "\n",
    "    for i in range(len(dataset_filtered)):\n",
    "        state = dataset_filtered['state'][i]\n",
    "        x_date = np.array(dataset_filtered['x_date'][i])\n",
    "        queue_data = np.array(dataset_filtered['y_ridership_change'][i])\n",
    "\n",
    "        time_index_of_min = np.argmin(queue_data)\n",
    "\n",
    "        t4 = int(dataset_filtered[t4_cutoff][i])+1\n",
    "        t2 = int(time_index_of_min)-1\n",
    "\n",
    "        queue_data = queue_data[:t4+1]\n",
    "\n",
    "        Qmax = queue_data[t2]\n",
    "\n",
    "        initial = queue_data[1]\n",
    "        Delta = queue_data[-1]\n",
    "        alpha = (-6 * Qmax) / (t2 ** 3)\n",
    "        beta = (6 * (Qmax - Delta)) / ((t4 - t2) ** 3)\n",
    "        # Calculate Qmax, alpha, beta, etc. using modified_deviation\n",
    "\n",
    "        Qmax_values[(mobility_type, i)] = Qmax\n",
    "        alpha_values[(mobility_type, i)] = alpha\n",
    "        beta_values[(mobility_type, i)] = beta\n",
    "        initial_values[(mobility_type, i)] = initial\n",
    "        delta_values[(mobility_type, i)] = Delta\n",
    "\n",
    "        h_t_prefix = np.full(t2, np.nan)\n",
    "        t = np.array(range(len(queue_data[t2:t4+1])))\n",
    "\n",
    "        # Original Data\n",
    "        F_t_suffix = np.array(queue_data[t2:t4+1])\n",
    "        F_t = np.concatenate((h_t_prefix, F_t_suffix))\n",
    "\n",
    "        # S_t calculation\n",
    "        S_t_suffix = 1 - F_t_suffix\n",
    "        S_t = np.concatenate((h_t_prefix, S_t_suffix))\n",
    "\n",
    "        # Derivative (Gradient)\n",
    "        f_t_suffix = np.gradient(F_t_suffix)\n",
    "        f_t = np.concatenate((h_t_prefix, f_t_suffix))\n",
    "\n",
    "        # Calculate h_t\n",
    "        h_t_suffix = f_t_suffix / (1 - F_t_suffix)\n",
    "        h_t = np.concatenate((h_t_prefix, h_t_suffix))\n",
    "\n",
    "        # 1. Polynomial Fit\n",
    "        coeffs = np.polyfit(t, f_t_suffix, 2)\n",
    "        polynomial_fit = np.poly1d(coeffs)\n",
    "        fitted_f_t_poly_suffix = polynomial_fit(t)\n",
    "        fitted_f_t_poly = np.concatenate((h_t_prefix, fitted_f_t_poly_suffix))\n",
    "\n",
    "        # Calculate h_t using the polynomial fitted f_t\n",
    "        h_t_poly_suffix = fitted_f_t_poly_suffix / (1 - F_t_suffix)\n",
    "        h_t_poly = np.concatenate((h_t_prefix, h_t_poly_suffix))\n",
    "\n",
    "        # 2. Moving Average\n",
    "        f_t_series = pd.Series(f_t_suffix)\n",
    "        fitted_f_t_ma_suffix = f_t_series.rolling(\n",
    "            window=7, min_periods=1).mean().values\n",
    "        fitted_f_t_ma = np.concatenate((h_t_prefix, fitted_f_t_ma_suffix))\n",
    "\n",
    "        # Calculate h_t using the moving average fitted f_t\n",
    "        h_t_ma_suffix = fitted_f_t_ma_suffix / (1 - F_t_suffix)\n",
    "        h_t_ma = np.concatenate((h_t_prefix, h_t_ma_suffix))\n",
    "\n",
    "        # Padding to 250\n",
    "        num_zeros_to_append = 250 - len(h_t)\n",
    "        h_t = np.concatenate((h_t, np.full(num_zeros_to_append, np.nan)))\n",
    "        f_t = np.concatenate((f_t, np.full(num_zeros_to_append, np.nan)))\n",
    "        S_t = np.concatenate((S_t, np.full(num_zeros_to_append, np.nan)))\n",
    "        F_t = np.concatenate((F_t, np.full(num_zeros_to_append, np.nan)))\n",
    "\n",
    "        fitted_f_t_poly = np.concatenate(\n",
    "            (fitted_f_t_poly, np.full(num_zeros_to_append, np.nan)))\n",
    "        h_t_poly = np.concatenate(\n",
    "            (h_t_poly, np.full(num_zeros_to_append, np.nan)))\n",
    "        fitted_f_t_ma = np.concatenate(\n",
    "            (fitted_f_t_ma, np.full(num_zeros_to_append, np.nan)))\n",
    "        h_t_ma = np.concatenate((h_t_ma, np.full(num_zeros_to_append, np.nan)))\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'state': [state] * 250,\n",
    "            'mobility_type': [mobility_type] * 250,\n",
    "            \"t\": np.arange(250),\n",
    "            'h(t)': h_t,\n",
    "            'f(t)': f_t,\n",
    "            'S(t)': S_t,\n",
    "            'F(t)': F_t,\n",
    "            'f(t)_poly': fitted_f_t_poly,\n",
    "            'h(t)_poly': h_t_poly,\n",
    "            'f(t)_ma': fitted_f_t_ma,\n",
    "            'h(t)_ma': h_t_ma,\n",
    "            'Qmax': [Qmax] * 250,\n",
    "            'Delta': [Delta] * 250\n",
    "        })\n",
    "\n",
    "        # Append the temporary DataFrame to the main long format DataFrame\n",
    "        df_long_format = pd.concat(\n",
    "            [df_long_format, df], ignore_index=True)\n",
    "\n",
    "\n",
    "df_long_format.dropna(inplace=True)\n",
    "\n",
    "df_long_format.to_csv(\n",
    "    f'Results/Calculation Results/calculation_results_{t4_cutoff}.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
